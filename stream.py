from modelscope import AutoTokenizer
from modelscope import AutoModelForCausalLM, BitsAndBytesConfig
from transformers import TextStreamer # <--- å¼•å…¥æµå¼è¾“å‡ºå·¥å…·
import torch
import gc

model_name = "Qwen/Qwen3-14B"

print(f"ðŸš€ æ­£åœ¨åŠ è½½ Tokenizer: {model_name} ...")
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)

print("âš™ï¸  æ­£åœ¨é…ç½® 8-bit é‡åŒ–æ¨¡å¼...")
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True
)

print("ðŸ“¥ æ­£åœ¨åŠ è½½æ¨¡åž‹ (å¼ºåˆ¶ä½¿ç”¨ GPU)...")
# å¼ºåˆ¶æŒ‡å®š device_map="cuda:0"ï¼Œç¡®ä¿ 100% è·‘åœ¨æ˜¾å¡ä¸Š
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cuda:0", 
    quantization_config=bnb_config,
    low_cpu_mem_usage=True,
    trust_remote_code=True
)

# æ¸…ç†æ˜¾å­˜
print("ðŸ§¹ æ­£åœ¨æ¸…ç†ä¸´æ—¶æ˜¾å­˜...")
gc.collect()
torch.cuda.empty_cache()

print(f"âœ… åŠ è½½å®Œæˆï¼å½“å‰æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

# ================= å®šä¹‰æµå¼è¾“å‡ºå™¨ =================
# å®ƒçš„ä½œç”¨æ˜¯ï¼šç”Ÿæˆä¸€ä¸ªå­—ï¼Œå°±æ‰“å°ä¸€ä¸ªå­—ï¼Œä¸ç”¨ç­‰å…¨éƒ¨ç”Ÿæˆå®Œ
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

messages = []
print("ðŸ’¡ ç³»ç»Ÿæç¤º: è¾“å…¥ 'exit' é€€å‡ºï¼Œè¾“å…¥ 'clear' æ¸…ç©ºåŽ†å²ã€‚")

while True:
    try:
        user_input = input("\nðŸ‘¤ User: ").strip()
    except EOFError:
        break

    if not user_input: continue
    if user_input.lower() in ['exit', 'quit', 'q']: break
    if user_input.lower() == 'clear':
        messages = []
        gc.collect()
        torch.cuda.empty_cache()
        print("ðŸ§¹ åŽ†å²å·²æ¸…ç©º")
        continue

    messages.append({"role": "user", "content": user_input})

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    print("ðŸ¤– Assistant: ", end="", flush=True) # æ‰“å°ä¸ªå¤´ï¼ŒåŽé¢æŽ¥ç€æµå¼è¾“å‡º
    
    # å¼€å§‹æŽ¨ç†
    # æ³¨æ„ï¼šè¿™é‡ŒæŠŠç”Ÿæˆçš„ id ä¹Ÿä¸è¦äº†ï¼Œå› ä¸º streamer ä¼šè‡ªåŠ¨æ‰“å°åˆ°å±å¹•ä¸Š
    generated_ids = model.generate(
        **inputs,
        max_new_tokens=2048,
        streamer=streamer, # <--- å…³é”®ï¼šæŠŠç»“æžœäº¤ç»™ streamer å¤„ç†
        temperature=0.7,
        top_p=0.9
    )

    # ä¸ºäº†ä¿æŒåŽ†å²è®°å½•ï¼Œæˆ‘ä»¬éœ€è¦æŠŠç”Ÿæˆçš„å†…å®¹æ‹¿å›žæ¥å­˜è¿› messages
    # è¿™é‡Œçš„é€»è¾‘ç¨æ˜¾å¤æ‚ï¼Œæ˜¯ä¸ºäº†ä»Ž output é‡Œæå–å‡ºçº¯å›žå¤éƒ¨åˆ†
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
    ]
    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    
    messages.append({"role": "assistant", "content": response})