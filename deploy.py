from modelscope import AutoTokenizer
from modelscope import AutoModelForCausalLM, BitsAndBytesConfig
import torch
import gc

# ================= é…ç½®åŒºåŸŸ =================
model_name = "Qwen/Qwen3-14B" # è¿™é‡Œçš„è·¯å¾„ä¿æŒä½ åŸæ¥çš„
# ===========================================

print(f"ğŸš€ æ­£åœ¨åŠ è½½ Tokenizer: {model_name} ...")
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)

# 1. æ”¹ä¸º 8-bit é‡åŒ–é…ç½®
# 8-bit ä¸éœ€è¦ nf4 ç­‰å‚æ•°ï¼Œåªéœ€è¦ load_in_8bit=True
print("âš™ï¸  æ­£åœ¨é…ç½® 8-bit é‡åŒ–æ¨¡å¼...")
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True
)

print("ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° GPU (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
    low_cpu_mem_usage=True,
    trust_remote_code=True
)

# 2. å…³é”®æ­¥éª¤ï¼šåŠ è½½å®Œåç«‹å³æ¸…ç†æ˜¾å­˜
# è¿™ä¼šæŠŠåŠ è½½è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸´æ—¶ç¢ç‰‡æ¸…ç†æ‰ï¼Œè…¾å‡ºç©ºé—´ç»™æ¨ç†
print("ğŸ§¹ æ­£åœ¨æ¸…ç†åŠ è½½äº§ç”Ÿçš„ä¸´æ—¶æ˜¾å­˜...")
gc.collect()
torch.cuda.empty_cache()

# æ‰“å°å½“å‰æ˜¾å­˜çŠ¶æ€
mem_alloc = torch.cuda.memory_allocated() / 1024**3
mem_reserved = torch.cuda.memory_reserved() / 1024**3
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
print(f"ğŸ“Š å½“å‰æ˜¾å­˜å®é™…å ç”¨: {mem_alloc:.2f} GB")
print(f"ğŸ“Š å½“å‰æ˜¾å­˜é¢„ç•™æ€»é‡: {mem_reserved:.2f} GB")
print("-" * 50)

# ================= å¾ªç¯å¯¹è¯é€»è¾‘ =================
# ç”¨äºå­˜å‚¨å†å²å¯¹è¯ï¼Œå®ç°â€œå¤šè½®å¯¹è¯â€
messages = []

print("ğŸ’¡ ç³»ç»Ÿæç¤º: è¾“å…¥ 'exit' æˆ– 'q' é€€å‡ºå¯¹è¯ï¼Œè¾“å…¥ 'clear' æ¸…ç©ºå†å²è®°å½•ã€‚")

while True:
    # è·å–ç”¨æˆ·è¾“å…¥
    try:
        user_input = input("\nğŸ‘¤ User: ").strip()
    except EOFError:
        break

    if not user_input:
        continue
    
    # é€€å‡ºå‘½ä»¤
    if user_input.lower() in ['exit', 'quit', 'q']:
        print("ğŸ‘‹ å†è§ï¼")
        break
        
    # æ¸…ç©ºå†å²å‘½ä»¤
    if user_input.lower() == 'clear':
        messages = []
        gc.collect()
        torch.cuda.empty_cache()
        print("ğŸ§¹ å†å²è®°å½•å·²æ¸…ç©ºï¼Œæ˜¾å­˜å·²æ•´ç†ã€‚")
        continue

    # å°†ç”¨æˆ·è¾“å…¥åŠ å…¥å†å²
    messages.append({"role": "user", "content": user_input})

    # åº”ç”¨èŠå¤©æ¨¡æ¿
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # 3. æ¨ç†ç”Ÿæˆ
    # 8-bit æ˜¾å­˜æ¯” 4-bit ç´§å¼ ï¼Œæ‰€ä»¥ max_new_tokens ä¸è¦è®¾å¾—å¤ªç–¯ç‹‚ï¼Œ2048 è¶³å¤Ÿæ—¥å¸¸å¯¹è¯
    # å¦‚æœæ˜¾å­˜çˆ†äº†ï¼Œå°è¯•è°ƒå° max_new_tokens æˆ–å®šæœŸè¾“å…¥ clear
    try:
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=2048,  
            temperature=0.7,
            top_p=0.9
        )
    except torch.cuda.OutOfMemoryError:
        print("âŒ æ˜¾å­˜ä¸è¶³ (OOM)ï¼æ­£åœ¨è‡ªåŠ¨æ¸…ç†å¹¶é‡ç½®å¯¹è¯...")
        gc.collect()
        torch.cuda.empty_cache()
        messages = [] # æ˜¾å­˜çˆ†äº†é€šå¸¸åªèƒ½æ¸…ç©ºå†å²
        continue

    # è·å–çº¯ç²¹çš„æ–°ç”Ÿæˆå†…å®¹ï¼ˆå»æ‰è¾“å…¥çš„ prompt éƒ¨åˆ†ï¼‰
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    
    # æ‰“å°å›ç­”
    print(f"ğŸ¤– Assistant: {response}")

    # å°† AI çš„å›ç­”ä¹ŸåŠ å…¥å†å²ï¼Œå½¢æˆä¸Šä¸‹æ–‡
    messages.append({"role": "assistant", "content": response})