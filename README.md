# Qwen3-14B 测试用例生成助手项目

##  项目简介

这是一个基于 **Qwen3-14B** 大语言模型的测试用例生成助手项目。通过微调训练，让AI学会根据功能描述自动生成专业的测试用例JSON格式数据。

###  项目目标
- **训练AI模型**：让AI学会理解功能描述并生成标准化的测试用例
- **部署AI服务**：提供多种方式使用训练好的AI模型
- **降低测试成本**：自动化测试用例编写，提高软件测试效率

---

##  项目文件结构说明

###  部署相关文件（用于运行和使用AI模型）

| 文件 | 用途 | 适合谁使用 |
|------|------|------------|
| `deploy.py` | **命令行对话模式** - 在终端与AI聊天 | 想简单测试AI功能的用户 |
| `server.py` | **Web API服务** - 提供HTTP接口供其他程序调用 | 开发者，需要集成AI到其他系统 |
| `stream.py` | **流式对话模式** - 实时显示AI回答过程 | 喜欢看到AI逐字回答的用户 |
| `test.py` | **API测试工具** - 测试Web服务是否正常 | 开发者，测试API接口 |

###  微调相关文件（用于训练AI模型）

| 文件 | 用途 | 适合谁使用 |
|------|------|------------|
| `pure_finetune.py` | **基础微调脚本** - 使用标准方法训练AI | AI开发者，有深度学习经验 |
| `run_finetune.py` | **高级微调脚本** - 使用Swift框架训练 | AI开发者，需要更多优化功能，显存需求大一点 |
| `train.jsonl` | **训练数据** - 包含功能描述和对应的测试用例 | 数据准备人员 |

###  测试相关文件

| 文件 | 用途 | 适合谁使用 |
|------|------|------------|
| `test_model.py` | **模型测试脚本** - 验证训练效果 | AI开发者，检查训练结果 |

###  输出目录

| 目录 | 内容 | 说明 |
|------|------|------|
| `output/` | 训练过程中的临时模型 | 多个版本的训练结果 |
| `output_final/` | 最终训练好的模型 | 可以部署使用的AI模型 |

---

##  如何使用这个项目

### 1 快速开始（最简单的方式）

如果你只是想**使用**训练好的AI模型：

```bash
# 方式1：命令行对话（推荐新手）
python deploy.py

# 方式2：流式对话（看到AI逐字思考）
python stream.py
```

运行后会看到：
```
🚀 正在加载 Tokenizer: Qwen/Qwen3-14B ...
⚙️  正在配置 8-bit 量化模式...
📥 正在加载模型到 GPU...
🤖 系统提示: 输入 'exit' 退出，输入 'clear' 清空历史
👤 User: 
```

### 2启动Web服务（供其他程序调用）

```bash
# 启动API服务器
python server.py

# 在另一个终端测试API
python test.py
```

服务器启动后，可以通过 `http://127.0.0.1:8000` 访问AI服务。

### 3 测试训练好的模型

```bash
# 测试微调后的AI效果
python test_model.py
```

---

##  这个AI能做什么？

###  输入示例
```
功能名称：数据入库
功能描述：提供遥感监测数据、业务结构化数据、市场价格数据、其他文件数据的入库支持，支持文件数据导入、数据表导入等多种入库方式。
```

###  输出示例（AI自动生成）
```json
{
  "module": "数据入库",
  "function": [
    {
      "name": "文件数据导入",
      "desc": "支持遥感监测数据、业务结构化数据、市场价格数据、其他文件数据等以文件形式导入。",
      "testcase": [
        {
          "name": "导入有效的遥感监测数据文件",
          "pre": "准备一个格式正确、内容符合要求的遥感监测数据文件...",
          "step": "1. 进入数据入库模块...",
          "result": "文件成功导入，系统提示导入成功..."
        }
      ]
    }
  ]
}
```

---

##  技术细节（给开发者看）

### 模型配置
- **基础模型**: Qwen3-14B (140亿参数)
- **微调方法**: LoRA (低秩适应)
- **量化**: 4-bit/8-bit 量化，节省显存
- **训练数据**: 包含各种软件功能的测试用例

### 硬件要求
- **最低配置**: 16GB GPU显存
- **推荐配置**: 24GB+ GPU显存
- **量化版本**: 可在8GB显存运行

### 训练过程
1. **数据准备**: 整理功能描述和测试用例对
2. **模型加载**: 加载Qwen3-14B基础模型
3. **LoRA微调**: 只训练少量参数，保持原模型能力
4. **模型保存**: 生成可部署的微调模型

---

##  训练数据说明

`train.jsonl` 文件包含丰富的训练样本，涵盖：

- **数据管理类**: 数据入库、数据录入、数据检查
- **系统管理类**: 用户管理、权限管理、日志管理  
- **业务功能类**: 接口管理、指标管理、标准管理
- **数据分析类**: 统计分析、挖掘分析、数据图谱
- **农业应用类**: 农情监测、气象服务、生猪养殖

每个样本都包含完整的功能描述和对应的测试用例JSON。

---

## 快速部署方案

### 方案1：使用预训练模型（推荐新手）

项目已经包含了**微调好的模型**在 `output_final/` 目录中，可以直接使用：

```bash
# 1. 安装依赖
pip install -r requirements.txt

# 2. 运行命令行对话
python deploy.py

# 3. 或者启动Web服务
python server.py

# 4. 在另一个终端测试API
python test.py
```

### 方案2：重新训练模型

如果你想重新训练模型：

```bash
# 使用基础微调脚本
python pure_finetune.py

# 或者使用高级微调脚本
python run_finetune.py
```

### 方案3：使用流式对话

```bash
python stream.py
```

##  环境配置

### 硬件要求
- **最低配置**: 16GB GPU显存
- **推荐配置**: 24GB+ GPU显存
- **量化版本**: 可在8GB显存运行

### 软件要求
- **Python**: 3.8+
- **CUDA**: 11.8+
- **PyTorch**: 2.0+

##  依赖包确认

项目中的 `requirements.txt` 文件包含了**准确的版本信息**，基于实际安装的包生成：

```txt
# Core AI/ML libraries
torch==2.5.1+cu121
transformers==4.57.1
accelerate==1.11.0
peft==0.17.1
bitsandbytes==0.48.2

# ModelScope (for Qwen models)
modelscope==1.32.0

# Web framework for API
fastapi==0.121.3
uvicorn==0.38.0

# Data processing
datasets==3.6.0

# Swift framework (for advanced training)
ms-swift==3.10.2

# HTTP client for testing
requests==2.32.5
```

##  注意事项

### 使用前准备
1. **安装依赖**: `pip install -r requirements.txt`
2. **模型文件**: 项目已包含微调好的模型，无需额外下载
3. **GPU要求**: 需要NVIDIA显卡和CUDA支持

### 常见问题
- **显存不足**: 模型已使用量化，如果仍有问题可减少输入长度
- **端口占用**: Web服务默认使用8000端口，可修改server.py中的端口号
- **模型加载**: 首次运行可能需要几分钟加载模型

---

##  技术支持

如果遇到问题：
1. 检查控制台输出的错误信息
2. 确认GPU驱动和CUDA版本
3. 查看项目文档和示例代码
4. 在GitHub提交Issue

---

##  总结

这个项目展示了如何：
- ✅ 训练AI理解功能需求
- ✅ 自动生成专业测试用例  
- ✅ 部署AI为可用的服务
- ✅ 集成到现有开发流程

无论你是测试工程师、开发者还是AI爱好者，这个项目都能帮助你提高工作效率！
