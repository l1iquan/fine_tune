import torch
from threading import Thread
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig, 
    TextIteratorStreamer
)
from peft import PeftModel
import gc
import os

# ================= é…ç½®åŒºåŸŸ =================
model_path = r"F:\huggingface\models\Qwen\Qwen3-14B" 
lora_path = "output_final" # ä½ çš„å¾®è°ƒç»“æœè·¯å¾„
# ===========================================

# 1. æ˜¾å­˜å¤§æ‰«é™¤
gc.collect()
torch.cuda.empty_cache()

# 2. 4-bit é‡åŒ–é…ç½®
print("âš™ï¸  æ­£åœ¨é…ç½® 4-bit é‡åŒ–...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

print(f"ğŸš€ æ­£åœ¨åŠ è½½ Tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹ (å¼ºåˆ¶é™åˆ¶æ˜¾å­˜ç”¨æ³•)...")

# === ã€æ ¸å¿ƒä¿®å¤ã€‘ ===
# æˆ‘ä»¬ç»™æ˜¾å¡è®¾ä¸€ä¸ªâ€œè½¯ä¸Šé™â€ï¼š16GBã€‚
# 4-bit æ¨¡å‹æœ¬èº«çº¦ 9.5GBã€‚
# è®¾ç½® 16GBï¼Œæ—¢ä¿è¯æ¨¡å‹èƒ½è£…è¿›å»ï¼Œåˆå¼ºåˆ¶é˜»æ­¢å®ƒä¸€å¼€å§‹å°±å æ»¡ 24GBã€‚
# è¿™æ ·åŠ è½½å®Œåï¼Œä½ åº”è¯¥ä¼šçœ‹åˆ°æ˜¾å­˜å ç”¨åœ¨ 10GB - 11GB å·¦å³ã€‚
# å‰©ä¸‹çš„ 13GB æ˜¾å­˜ï¼Œæ‰æ˜¯ç•™ç»™æµå¼å¯¹è¯å’Œé•¿ä¸Šä¸‹æ–‡ç”¨çš„ï¼
max_memory_map = {0: "16GB", "cpu": "99GB"}

base_model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True,
    max_memory=max_memory_map  # <--- åŠ ä¸Šè¿™è¡Œæ•‘å‘½ä»£ç 
)

# 3. æŒ‚è½½ LoRA
print(f"ğŸ”„ æ­£åœ¨æŒ‚è½½ LoRA: {lora_path}")
model = PeftModel.from_pretrained(base_model, lora_path)

# æ‰“å°ä¸€ä¸‹çœŸå®çš„æ˜¾å­˜å ç”¨
print(f"âœ… åŠ è½½å®Œæˆï¼å½“å‰æ˜¾å­˜: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
# === æ–°å¢ï¼šå¼ºåˆ¶é‡Šæ”¾ PyTorch çš„ç¼“å­˜ç©ºæˆ¿ ===
print("ğŸ§¹ æ­£åœ¨æ¸…ç†åŠ è½½é˜¶æ®µäº§ç”Ÿçš„ç¼“å­˜ç¢ç‰‡...")
gc.collect()
torch.cuda.empty_cache() # <--- è¿™è¡Œå‘½ä»¤ä¼šæŠŠé‚£ 12GB ç©ºæˆ¿è¿˜ç»™ Windows
# ========================================

# å†æ‰“å°ä¸€æ¬¡ï¼Œä½ ä¼šå‘ç°ä»»åŠ¡ç®¡ç†å™¨çš„æ•°å€¼é™ä¸‹æ¥äº†
print(f"ğŸ“‰ æ¸…ç†åæ˜¾å­˜çŠ¶æ€ï¼š")
print(f"   - å®é™…æ¨¡å‹å ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
print(f"   - æ€»æ˜¾å­˜å ç”¨ (å«ç¼“å­˜): {torch.cuda.memory_reserved()/1024**3:.2f} GB")

print("-" * 60)

# ================= è¿ç»­å¯¹è¯ä¸»å¾ªç¯ =================
history = [] 

print("ğŸ’¡ è¾“å…¥å†…å®¹å¼€å§‹å¯¹è¯ã€‚æŒ‡ä»¤ï¼š'clear' æ¸…ç©ºï¼Œ'exit' é€€å‡ºã€‚")

while True:
    try:
        user_input = input("\nğŸ‘¤ User: ").strip()
    except EOFError:
        break

    if not user_input: continue
    
    if user_input.lower() in ['exit', 'quit', 'q']:
        break
    
    if user_input.lower() == 'clear':
        history = []
        gc.collect()
        torch.cuda.empty_cache()
        print("ğŸ§¹ è®°å¿†å·²æ¸…ç©º")
        continue

    # æ„å»º Prompt
    history.append({"role": "user", "content": user_input})
    
    input_str = tokenizer.apply_chat_template(
        history,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([input_str], return_tensors="pt").to(model.device)

    # æµå¼è¾“å‡ºé…ç½®
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    generation_kwargs = dict(
        inputs,
        streamer=streamer,
        max_new_tokens=512, # å•æ¬¡å›å¤æœ€å¤§é•¿åº¦
        temperature=0.7,
        do_sample=True
    )

    # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    # æ‰“å°è¾“å‡º
    print("ğŸ¤– Assistant: ", end="", flush=True)
    full_response = ""
    
    for new_text in streamer:
        print(new_text, end="", flush=True)
        full_response += new_text
    
    print() 

    # è®°å½•å†å²
    history.append({"role": "assistant", "content": full_response})
    
    # ç®€å•çš„æ˜¾å­˜ä¿æŠ¤ï¼šåªä¿ç•™æœ€è¿‘10è½®å¯¹è¯ï¼Œé˜²æ­¢å†å²å¤ªé•¿çˆ†æ˜¾å­˜
    if len(history) > 20:
        history = history[-20:]