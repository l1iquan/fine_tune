import asyncio
import json
import time
from threading import Thread

import torch
import gc
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from modelscope import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from transformers import TextIteratorStreamer # å¼•å…¥è¿­ä»£æµå¼å·¥å…·

# ================== 1. æ¨¡å‹åŠ è½½ (ä¿æŒä¸å˜) ==================
app = FastAPI(title="Qwen3-14B Stream API", description="Local LLM Server")

model_name = "Qwen/Qwen3-14B" # ä¿æŒä½ çš„è·¯å¾„

print(f"ğŸš€ æ­£åœ¨å¯åŠ¨æœåŠ¡ç«¯ï¼ŒåŠ è½½æ¨¡å‹: {model_name} ...")

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
bnb_config = BitsAndBytesConfig(load_in_8bit=True)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cuda:0",
    quantization_config=bnb_config,
    low_cpu_mem_usage=True,
    trust_remote_code=True
)

# æ¸…ç†æ˜¾å­˜
gc.collect()
torch.cuda.empty_cache()
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

# ================== 2. å®šä¹‰è¯·æ±‚ç»“æ„ ==================
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    messages: list[ChatMessage]
    max_tokens: int = 32768    # é»˜è®¤è®¾å¤§ä¸€ç‚¹ï¼Œé˜²æ­¢å›ç­”ä¸€åŠæ–­æ‰
    temperature: float = 0.7
    top_p: float = 0.9
    stream: bool = True       # é»˜è®¤å¼€å¯æµå¼
    enable_thinking: bool = False # é»˜è®¤å…³é—­æ€è€ƒæ¨¡å¼

# ================== 3. æ ¸å¿ƒé€»è¾‘ï¼šæµå¼ç”Ÿæˆå™¨ ==================
def stream_generation(inputs, streamer, max_tokens, temp, top_p):
    """åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œæ¨ç†ï¼ŒæŠŠç»“æœå–‚ç»™ streamer"""
    try:
        model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            streamer=streamer,
            temperature=temp,
            top_p=top_p,
            do_sample=True
        )
    except Exception as e:
        print(f"ç”Ÿæˆå‡ºé”™: {e}")

async def generate_stream_response(streamer):
    """å¼‚æ­¥è¯»å– streamer ä¸­çš„ token å¹¶æŒ‰ SSE æ ¼å¼å‘é€"""
    request_id = f"chatcmpl-{int(time.time())}"
    
    for new_text in streamer:
        if not new_text: continue
        
        # æ„å»º OpenAI å…¼å®¹çš„æµå¼æ•°æ®åŒ…
        chunk = {
            "id": request_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model_name,
            "choices": [{
                "index": 0,
                "delta": {"content": new_text},
                "finish_reason": None
            }]
        }
        # SSE æ ¼å¼è¦æ±‚ï¼šä»¥ data: å¼€å¤´ï¼ŒåŒæ¢è¡Œç»“å°¾
        yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0) # è®©å‡ºæ§åˆ¶æƒ

    # å‘é€ç»“æŸä¿¡å·
    end_chunk = {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model_name,
        "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
    }
    yield f"data: {json.dumps(end_chunk, ensure_ascii=False)}\n\n"
    yield "data: [DONE]\n\n"

# ================== 4. API æ¥å£ ==================
@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest):
    print(f"ğŸ“© æ”¶åˆ°è¯·æ±‚ (Stream={request.stream}, Thinking={request.enable_thinking})")
    
    # 1. è½¬æ¢æ¶ˆæ¯å¹¶åº”ç”¨æ¨¡æ¿
    msgs = [{"role": m.role, "content": m.content} for m in request.messages]
    
    # --- å…³é”®ä¿®æ”¹ï¼šåœ¨æ­¤å¤„å…³é—­æ€è€ƒæ¨¡å¼ ---
    # enable_thinking=False ä¼ ç»™æ¨¡æ¿ï¼Œé˜²æ­¢ç”Ÿæˆ <think> æ ‡ç­¾
    text = tokenizer.apply_chat_template(
        msgs,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=request.enable_thinking 
    )
    
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # 2. åˆå§‹åŒ–æµå¼è¿­ä»£å™¨
    # skip_prompt=True: ä¸é‡å¤æ‰“å°é—®é¢˜
    # skip_special_tokens=True: ä¸æ‰“å° <|endoftext|> ç­‰ç‰¹æ®Šç¬¦
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    # 3. å¯åŠ¨å­çº¿ç¨‹è¿›è¡Œæ¨ç† (è¿™æ˜¯æµå¼çš„å…³é”®ï¼)
    generation_kwargs = dict(
        inputs=inputs,
        streamer=streamer,
        max_tokens=request.max_tokens,
        temp=request.temperature,
        top_p=request.top_p
    )
    
    thread = Thread(target=stream_generation, kwargs=generation_kwargs)
    thread.start()

    # 4. å¦‚æœæ˜¯æµå¼è¯·æ±‚ï¼Œè¿”å› StreamingResponse
    if request.stream:
        return StreamingResponse(
            generate_stream_response(streamer), 
            media_type="text/event-stream"
        )
    
    # 5. å¦‚æœéæµå¼ (å…¼å®¹æ—§ä»£ç )ï¼Œç­‰å¾…çº¿ç¨‹ç»“æŸæ”¶é›†æ‰€æœ‰æ–‡æœ¬
    else:
        full_response = ""
        for new_text in streamer:
            full_response += new_text
        
        return {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model_name,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": full_response
                },
                "finish_reason": "stop"
            }]
        }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)